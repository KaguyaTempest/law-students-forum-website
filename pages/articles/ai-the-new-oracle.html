<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <base href="/law-students-forum-website/" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>The New Oracle: How AI Is Replacing Thought | LSIF</title>

  <link rel="icon" href="/assets/favicon.ico" type="image/x-icon" />
  <link rel="stylesheet" href="/styles/style.css" />
  <link rel="stylesheet" href="/styles/article.css" />
</head>
<body>

  <!-- site header -->
  <div id="header-placeholder"></div>

  <main class="article-main max-w-3xl mx-auto px-4 py-10">
    <!-- cover -->
    <img src="/assets/ai-oracle.png" alt="Cover image" class="w-full rounded mb-6" />

    <!-- title & meta -->
    <h1 class="text-3xl font-bold mb-2">The New Oracle: How AI Is Replacing Thought</h1>
    <p class="text-lg italic mb-6">
      An analysis of how artificial intelligence threatens to supplant religion as the tool of the bourgeoisie to control the masses
    </p>
    <p class="text-sm opacity-70 mb-10">
      By LMC Kingmaker, Kaguya Tempest • Published 4 July 2025
    </p>

    <!-- article body -->
    <h2>Introduction</h2>
    <p>Once upon a time, when people didn’t know the answer to life’s biggest questions: “Why are we here? What is justice? Should I take that job in Gweru?” they turned to religion. It was the ultimate authority. No need to think too hard; the truth had already been written, interpreted, and conveniently passed down by people in robes.</p>
    <p>Today? We’ve traded the robes for servers. Now, when faced with even mildly challenging tasks, from “What’s the difference between a delict and a tort?” to “What’s a good caption for my moot competition photo?”, the default reaction is: “Let’s just ask AI.”</p>
    <p>Artificial intelligence has become our new oracle. Tools like ChatGPT, Google Gemini, and Claude now answer everything from complex legal questions to what we should eat for lunch. And while that seems convenient, it also raises a serious concern: are we thinking less because the machine is thinking for us?</p>
    <p>A recent study by MIT Media Lab found that the more people rely on AI, the worse they perform on tasks after using it. In other words, AI might be helping in the moment, but it’s also quietly draining our brain batteries. That’s what researchers call <strong>cognitive debt</strong>.</p>
    <p>This article takes a closer look at how AI might be sneaking into the role religion once played not just as a source of answers, but as a replacement for critical thinking. Especially for us as law students, that’s a red flag. After all, if we give up thinking for ourselves, who’s really interpreting the law — the legal mind or the algorithm?</p>
    
    <h2>The eerie historic parallels between religion and control</h2>
    <p>In the ancient world, people didn’t Google their problems. They consulted oracles. From Delphi to Great Zimbabwe, the high priests and spirit mediums were the official middlemen between the mortal world and divine truth. If the harvest failed, if a war loomed, if the king fell ill — people turned to these oracles for answers. And those answers were rarely questioned. They shaped decisions, justified power, and upheld entire systems of rule. But here’s the thing: the gods didn’t actually speak. The interpreters did.</p>
    <p>Fast forward to today, and we see a similar pattern emerging with AI. Just as the oracles were the gatekeepers of divine knowledge, AI systems are becoming the gatekeepers of information. They provide answers, often with an air of authority, but they are ultimately just tools created by humans. And like the oracles, they can be manipulated, biased, and opaque.</p>
    <p>Fast-forward to the present, and the same dynamic is quietly resurfacing. Today, when uncertainty strikes — whether it’s about legal research, moral dilemmas, or what to do with your life after law school — people increasingly turn to artificial intelligence. The answers it gives feel objective, neutral, and strangely authoritative. Just like the voice of a god.</p>
    <p>But the real question is this: who is training the oracle this time?</p>
    <p>Every AI model is trained on data. That data is selected, filtered, and moderated by human beings, and behind those humans are institutions with ideologies, legal obligations, economic interests, and cultural assumptions. ChatGPT, for example, is trained on a massive dataset that leans heavily Western, secular, and sanitized for mainstream acceptability. It’s not <strong>neutral</strong>, it’s carefully curated.</p>
    <p>Imagine, for a second, if the same model were trained exclusively using Islamic jurisprudence, or Vatican doctrine, or Marxist-Leninist philosophy. You’d get vastly different answers to the same questions. AI doesn’t “know” things in the way a human being does — it predicts responses based on its training. In essence, it is a reflection of its creators’ choices.</p>
    <p>This is where the parallel with religion becomes deeply unsettling. Just as popes, priests, and colonial missionaries once determined the moral and legal boundaries of society, today’s AI creators are in a position to shape thought on a mass scale. Censorship, bias, and ideological framing are no longer just tools of the state or the church — they can now be embedded in the code.</p>
    <p>What happens when an entire generation starts outsourcing thought to a machine trained with blind spots?</p>
    
    <h2>Conclusion</h2>
    <p>AI isn’t evil. It’s powerful. But like all power, it demands scrutiny. As law students and future legal minds, our duty isn’t to fear it — it’s to understand it, question it, and ensure it doesn’t replace the very thinking that defines the rule of law. Blind reliance on algorithms is no different from blind faith in oracles. If anything, it’s more dangerous, because this oracle doesn’t wear robes — it wears a UI.</p>
    
    <h2>References</h2>
    <p>Binns, R. (2018). ‘Fairness in Machine Learning: Lessons from Political Philosophy.’ <em>Proceedings of the 2018 Conference on Fairness, Accountability and Transparency</em>, 149–159.</p>
    <p>MIT Media Lab. (2023). ‘Cognitive Offloading and AI: A Study on Human Reliance on Machine Outputs.’ <em>Journal of Human-Computer Interaction</em>, Vol. 39(4).</p>
    <p>OpenAI. (2024). ‘GPT-4 Technical Report.’ Retrieved from https://openai.com/research/gpt-4</p>
  </main>

  <!-- comments placeholder -->
  <section id="comments" class="max-w-3xl mx-auto px-4 py-10">
    <!-- comment list + form will be injected later -->
  </section>

  <footer class="text-center py-8 text-sm opacity-70">
    © 2025 Law Students Intellectual Forum
  </footer>

  <script type="module" src="/scripts/load-header.js"></script>
  <script type="module" src="/scripts/loader.js"></script>
  <!-- later: <script type="module" src="/scripts/article-comments.js"></script> -->
</body>
</html>